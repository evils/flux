//! Bootstrap provides an API for compiling the Flux standard library.
//!
//! This package does not assume a location of the source code but does assume which packages are
//! part of the prelude.

use std::{env::consts, fs, io, io::Write, path::Path, sync::Arc};

use anyhow::{anyhow, bail, Result};
use libflate::gzip::Encoder;
use walkdir::WalkDir;

use crate::{
    ast,
    db::{Database, Flux},
    semantic::{
        env::Environment,
        flatbuffers::types::{build_module, finish_serialize},
        fs::{FileSystemImporter, StdFS},
        import::{Importer, Packages},
        nodes::{self, Package, Symbol},
        sub::{Substitutable, Substituter},
        types::{
            BoundTvar, BoundTvarKinds, MonoType, PolyType, PolyTypeHashMap, Record, RecordLabel,
            SemanticMap, Tvar,
        },
        Analyzer, AnalyzerConfig, PackageExports,
    },
};

// List of packages to include into the Flux prelude
const PRELUDE: [&str; 4] = [
    "internal/boolean",
    "internal/location",
    "universe",
    "influxdata/influxdb",
];

/// A mapping of package import paths to the corresponding AST package.
pub type ASTPackageMap = SemanticMap<String, ast::Package>;
/// A mapping of package import paths to the corresponding semantic graph package.
pub type SemanticPackageMap = SemanticMap<String, Package>;

/// Infers the Flux standard library given the path to the source code.
/// The prelude and the imports are returned.
#[allow(clippy::type_complexity)]
pub fn infer_stdlib_dir(
    path: &Path,
    config: AnalyzerConfig,
) -> Result<(PackageExports, Packages, SemanticPackageMap)> {
    let (db, package_list) = parse_dir(path)?;

    let mut infer_state = InferState {
        config,
        ..InferState::default()
    };
    let prelude = infer_state.infer_pre(&db)?;
    infer_state.infer_std(&db, &package_list, &prelude)?;

    Ok((prelude, infer_state.imports, infer_state.sem_pkg_map))
}

/// Recursively parse all flux files within a directory.
pub fn parse_dir(dir: &Path) -> io::Result<(Database, Vec<String>)> {
    let mut db = Database::default();
    let mut files = Vec::new();
    let entries = WalkDir::new(dir)
        .into_iter()
        .filter_map(|r| r.ok())
        .filter(|r| r.path().is_file());

    let is_windows = consts::OS == "windows";

    for entry in entries {
        if let Some(path) = entry.path().to_str() {
            if path.ends_with(".flux") && !path.ends_with("_test.flux") {
                let mut normalized_path = path.to_string();
                if is_windows {
                    // When building on Windows, the paths generated by WalkDir will
                    // use `\` instead of `/` as their separator. It's easier to normalize
                    // the separators to always be `/` here than it is to change the
                    // rest of this buildscript & the flux runtime initialization logic
                    // to work with either separator.
                    normalized_path = normalized_path.replace('\\', "/");
                }
                let source = Arc::<str>::from(fs::read_to_string(entry.path())?);

                let file_name = normalized_path
                    .rsplitn(2, "/stdlib/")
                    .collect::<Vec<&str>>()[0]
                    .to_owned();
                let path = file_name.rsplitn(2, '/').collect::<Vec<&str>>()[1].to_string();
                files.push(path.clone());
                db.set_source(path, source.clone());
            }
        }
    }
    Ok((db, files))
}

#[derive(Default)]
struct InferState {
    // types available for import
    imports: Packages,
    sem_pkg_map: SemanticPackageMap,
    config: AnalyzerConfig,
}

impl InferState {
    fn infer_pre(&mut self, ast_packages: &Database) -> Result<PackageExports> {
        let mut prelude_map = PackageExports::new();
        for name in PRELUDE {
            // Infer each package in the prelude allowing the earlier packages to be used by later
            // packages within the prelude list.
            let (types, _sem_pkg) = self.infer_pkg(name, ast_packages, &prelude_map)?;

            prelude_map.copy_bindings_from(&types);
        }
        Ok(prelude_map)
    }

    #[allow(clippy::type_complexity)]
    fn infer_std(
        &mut self,
        ast_packages: &Database,
        package_list: &[String],
        prelude: &PackageExports,
    ) -> Result<()> {
        for path in package_list {
            // No need to infer the package again if it has already been inferred through a
            // dependency
            if !self.sem_pkg_map.contains_key(path) {
                let (types, sem_pkg) = self.infer_pkg(path, ast_packages, prelude)?;

                self.sem_pkg_map.insert(path.to_string(), sem_pkg);
                if !self.imports.contains_key(path) {
                    self.imports.insert(path.to_string(), types);
                }
            }
        }
        Ok(())
    }

    // Infer the types in a package(file), returning a hash map containing
    // the inferred types along with a possibly updated map of package imports.
    //
    #[allow(clippy::type_complexity)]
    fn infer_pkg(
        &mut self,
        name: &str,               // name of package to infer
        db: &Database,            // ast_packages available for inference
        prelude: &PackageExports, // prelude types
    ) -> Result<(
        PackageExports, // inferred types
        Package,        // semantic graph
    )> {
        let file = db
            .ast_package(name.into())
            .ok_or_else(|| anyhow!(r#"package import "{}" not found"#, name))?;

        let env = Environment::new(prelude.into());
        let mut analyzer = Analyzer::new(env, &mut self.imports, self.config.clone());
        let (exports, sem_pkg) = analyzer.analyze_ast(&file).map_err(|mut err| {
            if err.error.source.is_none() {
                err.error.source = file.base.location.source.clone();
            }
            err.error.pretty_error()
        })?;

        Ok((exports, sem_pkg))
    }
}

fn stdlib_importer(path: &Path) -> FileSystemImporter<StdFS> {
    let fs = StdFS::new(path);
    FileSystemImporter::new(fs)
}

fn prelude_from_importer<I>(importer: &mut I) -> Result<PackageExports>
where
    I: Importer,
{
    let mut env = PolyTypeHashMap::new();
    for pkg in PRELUDE {
        if let Some(pkg_type) = importer.import(pkg) {
            if let MonoType::Record(typ) = pkg_type.expr {
                add_record_to_map(&mut env, typ.as_ref(), &pkg_type.vars, &pkg_type.cons)?;
            } else {
                bail!("package type is not a record");
            }
        } else {
            bail!("prelude package {} not found", pkg);
        }
    }
    let exports = PackageExports::try_from(env)?;
    Ok(exports)
}

// Collects any `MonoType::BoundVar`s in the type
struct CollectBoundVars(Vec<BoundTvar>);

impl Substituter for CollectBoundVars {
    fn try_apply(&mut self, _var: Tvar) -> Option<MonoType> {
        None
    }

    fn try_apply_bound(&mut self, var: BoundTvar) -> Option<MonoType> {
        let vars = &mut self.0;
        if let Err(i) = vars.binary_search(&var) {
            vars.insert(i, var);
        }
        None
    }
}

fn add_record_to_map(
    env: &mut PolyTypeHashMap<Symbol>,
    r: &Record,
    free_vars: &[BoundTvar],
    cons: &BoundTvarKinds,
) -> Result<()> {
    for field in r.fields() {
        let new_vars = {
            let mut new_vars = CollectBoundVars(Vec::new());
            field.v.visit(&mut new_vars);
            new_vars.0
        };

        let mut new_cons = BoundTvarKinds::new();
        for var in &new_vars {
            if !free_vars.iter().any(|v| v == var) {
                bail!("monotype contains free var not in poly type free vars");
            }
            if let Some(con) = cons.get(var) {
                new_cons.insert(*var, con.clone());
            }
        }
        env.insert(
            match &field.k {
                RecordLabel::Concrete(s) => s.clone().into(),
                RecordLabel::BoundVariable(_) | RecordLabel::Variable(_) => {
                    bail!("Record contains variable labels")
                }
                RecordLabel::Error => {
                    bail!("Record contains type error")
                }
            },
            PolyType {
                vars: new_vars,
                cons: new_cons,
                expr: field.v.clone(),
            },
        );
    }
    Ok(())
}

/// Stdlib returns the prelude and importer for the Flux standard library given a path to a
/// compiled directory structure.
pub fn stdlib(dir: &Path) -> Result<(PackageExports, FileSystemImporter<StdFS>)> {
    let mut stdlib_importer = stdlib_importer(dir);
    let prelude = prelude_from_importer(&mut stdlib_importer)?;
    Ok((prelude, stdlib_importer))
}

/// Compiles the stdlib found at the srcdir into the outdir.
pub fn compile_stdlib(srcdir: &Path, outdir: &Path) -> Result<()> {
    let (_, imports, mut sem_pkgs) = infer_stdlib_dir(srcdir, AnalyzerConfig::default())?;
    // Write each file as compiled module
    for (path, exports) in &imports {
        if let Some(code) = sem_pkgs.remove(path) {
            let module = Module {
                polytype: Some(exports.typ()),
                code: Some(code),
            };
            let mut builder = flatbuffers::FlatBufferBuilder::new();
            let offset = build_module(&mut builder, module);
            let buf = finish_serialize(&mut builder, offset);

            // Write module contents to file
            let mut fpath = outdir.join(path);
            fpath.set_extension("fc");
            fs::create_dir_all(fpath.parent().unwrap())?;
            let file = fs::File::create(&fpath)?;
            let mut encoder = Encoder::new(file)?;
            encoder.write_all(buf)?;
            encoder.finish().into_result()?;
        } else {
            bail!("package {} missing code", &path);
        }
    }
    Ok(())
}

/// Module represenets the result of compiling Flux source code.
///
/// The polytype represents the type of the entire package as a record type.
/// The record properties represent the exported values from the package.
///
/// The package is the actual code of the package that can be used to execute the package.
///
/// This struct is experimental we anticipate it will change as we build more systems around
/// the concepts of modules.
pub struct Module {
    /// The polytype
    pub polytype: Option<PolyType>,
    /// The code
    pub code: Option<nodes::Package>,
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::{ast, parser, semantic::convert::convert_polytype};

    #[test]
    fn infer_program() -> Result<()> {
        let a = r#"
            f = (x) => x
        "#;
        let b = r#"
            import "a"

            builtin x : int

            y = a.f(x: x)
        "#;
        let c = r#"
            package c
            import "b"

            z = b.y
        "#;

        let mut db = Database::default();

        for (k, v) in [("a", a), ("b", b), ("c", c)] {
            db.set_source(k.into(), v.into());
        }
        let mut infer_state = InferState::default();
        let (types, _) = infer_state.infer_pkg("c", &db, &PackageExports::new())?;

        let want = PackageExports::try_from(vec![(types.lookup_symbol("z").unwrap().clone(), {
            let mut p = parser::Parser::new("int");
            let typ_expr = p.parse_type_expression();
            if let Err(err) = ast::check::check(ast::walk::Node::TypeExpression(&typ_expr)) {
                panic!("TypeExpression parsing failed for int. {:?}", err);
            }
            convert_polytype(&typ_expr, &Default::default())?
        })])
        .unwrap();
        if want != types {
            bail!(
                "unexpected inference result:\n\nwant: {:?}\n\ngot: {:?}",
                want,
                types,
            );
        }

        let want = semantic_map! {
            String::from("a") => {
                let mut p = parser::Parser::new("{f: (x: A) => A}");
                let typ_expr = p.parse_type_expression();
                if let Err(err) = ast::check::check(ast::walk::Node::TypeExpression(&typ_expr)) {
                    panic!(
                        "TypeExpression parsing failed for int. {:?}", err
                    );
                }
                convert_polytype(&typ_expr, &Default::default())?
            },
            String::from("b") => {
                let mut p = parser::Parser::new("{x: int , y: int}");
                let typ_expr = p.parse_type_expression();
                if let Err(err) = ast::check::check(ast::walk::Node::TypeExpression(&typ_expr)) {
                    panic!(
                        "TypeExpression parsing failed for int. {:?}", err
                    );
                }
                convert_polytype(&typ_expr, &Default::default())?
            },
        };
        if want
            != infer_state
                .imports
                .into_iter()
                .map(|(k, v)| (k, v.typ()))
                .collect::<SemanticMap<_, _>>()
        {
            bail!(
                "unexpected type importer:\n\nwant: {:?}\n\ngot: {:?}",
                want,
                types,
            );
        }

        Ok(())
    }

    #[test]
    fn cyclic_dependency() {
        let a = r#"
            import "b"
        "#;
        let b = r#"
            import "a"
        "#;

        let mut db = Database::default();

        for (k, v) in [("a", a), ("b", b)] {
            db.set_source(k.into(), v.into());
        }

        let got_err = db
            .semantic_package("b".into())
            .expect_err("expected cyclic dependency error");

        assert_eq!(
            r#"package "b" depends on itself"#.to_string(),
            got_err.to_string(),
        );
    }
}
